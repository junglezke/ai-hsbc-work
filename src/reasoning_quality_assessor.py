"""
æ¨ç†è´¨é‡è¯„ä¼°å™¨ - ä¸“é—¨è¯„ä¼°ç”Ÿæˆçš„reasoning_traceè´¨é‡
"""
import json
import re
from typing import Dict, List, Any, Tuple
from collections import Counter


class ReasoningQualityAssessor:
    """æ¨ç†è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.quality_frameworks = self._load_quality_frameworks()
        
    def _load_quality_frameworks(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ä¸åŒç±»å‹å†…å®¹çš„è´¨é‡è¯„ä¼°æ¡†æ¶"""
        return {
            'qa_function': {
                'min_length': 150,
                'required_elements': ['é—®é¢˜åˆ†æ', 'æŠ€æœ¯è€ƒå¯Ÿ', 'æ·±åº¦æ¨ç†', 'å®è·µæ´å¯Ÿ'],
                'quality_indicators': [
                    'åˆ†æ', 'è€ƒè™‘', 'è¯„ä¼°', 'æ¨ç†', 'å› ä¸º', 'æ‰€ä»¥',
                    'èƒŒæ™¯', 'åŸå› ', 'å½±å“', 'ä¼˜åŠ¿', 'åŠ£åŠ¿', 'æ–¹æ¡ˆ',
                    'è®¾è®¡', 'æ¶æ„', 'æ¨¡å¼', 'åŸåˆ™', 'å®è·µ'
                ],
                'structure_patterns': ['1.', '2.', '3.', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'æ­¥éª¤', 'é˜¶æ®µ']
            },
            'qa_class': {
                'min_length': 200,
                'required_elements': ['è®¾è®¡æ„å›¾', 'æ¶æ„æ¨¡å¼', 'èŒè´£è¾¹ç•Œ', 'ä¾èµ–å…³ç³»', 'æ‰©å±•æ€§', 'æœ€ä½³å®è·µ'],
                'quality_indicators': [
                    'è®¾è®¡', 'æ¶æ„', 'æ¨¡å¼', 'èŒè´£', 'è€¦åˆ', 'æ‰©å±•',
                    'ç»´æŠ¤', 'åŸåˆ™', 'åˆ†æ', 'è¯„ä¼°', 'è€ƒè™‘'
                ],
                'structure_patterns': ['1.', '2.', '3.', '**', 'SOLID', 'å•ä¸€èŒè´£']
            },
            'qa_business': {
                'min_length': 180,
                'required_elements': ['ä¸šåŠ¡èƒŒæ™¯', 'è§„åˆ™é€»è¾‘', 'ä¸šåŠ¡ä»·å€¼', 'å½±å“èŒƒå›´', 'å¼‚å¸¸æƒ…å†µ', 'ä¼˜åŒ–å»ºè®®'],
                'quality_indicators': [
                    'ä¸šåŠ¡', 'è§„åˆ™', 'é€»è¾‘', 'ä»·å€¼', 'æµç¨‹', 'å½±å“',
                    'å¼‚å¸¸', 'å¤„ç†', 'ä¼˜åŒ–', 'å»ºè®®', 'åˆ†æ'
                ],
                'structure_patterns': ['1.', '2.', '3.', 'ä¸šåŠ¡', 'æµç¨‹', 'è§„åˆ™']
            },
            'qa_architecture': {
                'min_length': 220,
                'required_elements': ['æ¶æ„æ¨¡å¼', 'è®¾è®¡åŸåˆ™', 'å¯æ‰©å±•æ€§', 'æ€§èƒ½å½±å“', 'ç»´æŠ¤æ€§', 'æ¼”è¿›ç­–ç•¥'],
                'quality_indicators': [
                    'æ¶æ„', 'æ¨¡å¼', 'è®¾è®¡', 'åŸåˆ™', 'æ‰©å±•', 'æ€§èƒ½',
                    'ç»´æŠ¤', 'æ¼”è¿›', 'ç³»ç»Ÿ', 'SOLID', 'DRY', 'KISS'
                ],
                'structure_patterns': ['1.', '2.', '3.', 'SOLID', 'æ°´å¹³æ‰©å±•', 'å‚ç›´æ‰©å±•']
            },
            'design_enhancement': {
                'min_length': 400,
                'required_elements': ['ç°çŠ¶åˆ†æ', 'é—®é¢˜è¯†åˆ«', 'æ–¹æ¡ˆå¯¹æ¯”', 'æŠ€æœ¯é€‰å‹', 'é£é™©è¯„ä¼°', 'å®æ–½ç­–ç•¥', 'æˆåŠŸæ ‡å‡†'],
                'quality_indicators': [
                    'ç°çŠ¶', 'åˆ†æ', 'é—®é¢˜', 'æ ¹å› ', 'æ–¹æ¡ˆ', 'å¯¹æ¯”', 'é€‰æ‹©',
                    'æŠ€æœ¯', 'é£é™©', 'è¯„ä¼°', 'å®æ–½', 'ç­–ç•¥', 'æ ‡å‡†', 'è€ƒé‡',
                    'ä¼˜åŠ¿', 'åŠ£åŠ¿', 'ç¼“è§£', 'æªæ–½', 'æŒ‡æ ‡', 'æ¶æ„'
                ],
                'structure_patterns': ['1.', '2.', '3.', '**', 'ç°çŠ¶', 'é—®é¢˜', 'æ–¹æ¡ˆ', 'æŠ€æœ¯', 'é£é™©']
            }
        }
    
    def assess_reasoning_quality(self, content: Dict[str, Any], content_type: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªå†…å®¹çš„æ¨ç†è´¨é‡"""
        reasoning = content.get('reasoning_trace', '')
        framework = self.quality_frameworks.get(content_type, self.quality_frameworks['qa_function'])
        
        # 1. é•¿åº¦è¯„ä¼°
        length_score = min(len(reasoning) / framework['min_length'], 1.0)
        
        # 2. è´¨é‡æŒ‡æ ‡è¯„ä¼°
        quality_score = self._assess_quality_indicators(reasoning, framework['quality_indicators'])
        
        # 3. ç»“æ„åŒ–è¯„ä¼°
        structure_score = self._assess_structure(reasoning, framework['structure_patterns'])
        
        # 4. å…ƒç´ å®Œæ•´æ€§è¯„ä¼°
        completeness_score = self._assess_completeness(reasoning, framework['required_elements'])
        
        # 5. é€»è¾‘è¿è´¯æ€§è¯„ä¼°
        coherence_score = self._assess_coherence(reasoning)
        
        # ç»¼åˆå¾—åˆ†
        overall_score = (
            length_score * 0.15 +
            quality_score * 0.25 +
            structure_score * 0.20 +
            completeness_score * 0.25 +
            coherence_score * 0.15
        )
        
        return {
            'overall_score': round(overall_score, 3),
            'length_score': round(length_score, 3),
            'quality_score': round(quality_score, 3),
            'structure_score': round(structure_score, 3),
            'completeness_score': round(completeness_score, 3),
            'coherence_score': round(coherence_score, 3),
            'reasoning_length': len(reasoning),
            'required_length': framework['min_length'],
            'passes_threshold': overall_score >= 0.7
        }
    
    def _assess_quality_indicators(self, reasoning: str, indicators: List[str]) -> float:
        """è¯„ä¼°è´¨é‡æŒ‡æ ‡è¯æ±‡çš„ä½¿ç”¨"""
        if not reasoning:
            return 0.0
        
        found_indicators = sum(1 for indicator in indicators if indicator in reasoning)
        return min(found_indicators / len(indicators), 1.0)
    
    def _assess_structure(self, reasoning: str, patterns: List[str]) -> float:
        """è¯„ä¼°ç»“æ„åŒ–ç¨‹åº¦"""
        if not reasoning:
            return 0.0
        
        structure_count = sum(1 for pattern in patterns if pattern in reasoning)
        return min(structure_count / 3, 1.0)  # è‡³å°‘éœ€è¦3ä¸ªç»“æ„åŒ–å…ƒç´ 
    
    def _assess_completeness(self, reasoning: str, required_elements: List[str]) -> float:
        """è¯„ä¼°å¿…è¦å…ƒç´ çš„å®Œæ•´æ€§"""
        if not reasoning:
            return 0.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ†æå…ƒç´ 
        element_count = 0
        for element in required_elements:
            # ä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…ç­–ç•¥
            if element in reasoning or any(word in reasoning for word in element.split()):
                element_count += 1
        
        return element_count / len(required_elements)
    
    def _assess_coherence(self, reasoning: str) -> float:
        """è¯„ä¼°é€»è¾‘è¿è´¯æ€§"""
        if not reasoning:
            return 0.0
        
        # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
        logical_connectors = [
            'å› ä¸º', 'æ‰€ä»¥', 'ç”±äº', 'å› æ­¤', 'ç„¶è€Œ', 'ä½†æ˜¯', 'å¹¶ä¸”',
            'åŒæ—¶', 'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'ç»¼ä¸Š', 'æ€»ç»“', 'åŸºäº'
        ]
        
        connector_count = sum(1 for connector in logical_connectors if connector in reasoning)
        
        # æ£€æŸ¥å¥å­å®Œæ•´æ€§ï¼ˆé€šè¿‡æ ‡ç‚¹ç¬¦å·ï¼‰
        sentences = len([s for s in re.split(r'[ã€‚ï¼ï¼Ÿ.]', reasoning) if s.strip()])
        
        # é€»è¾‘è¿è´¯æ€§å¾—åˆ†
        if sentences == 0:
            return 0.0
        
        connector_ratio = min(connector_count / max(sentences / 3, 1), 1.0)
        
        return connector_ratio
    
    def generate_quality_report(self, qa_pairs: List[Dict[str, Any]], 
                              design_proposals: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´ä½“è´¨é‡æŠ¥å‘Š"""
        
        # è¯„ä¼°QA pairs
        qa_scores = []
        qa_type_scores = {}
        
        for qa in qa_pairs:
            element_type = qa.get('metadata', {}).get('element_type', 'function')
            content_type = f'qa_{element_type}'
            
            score_result = self.assess_reasoning_quality(qa, content_type)
            qa_scores.append(score_result)
            
            if content_type not in qa_type_scores:
                qa_type_scores[content_type] = []
            qa_type_scores[content_type].append(score_result['overall_score'])
        
        # è¯„ä¼°Design proposals
        design_scores = []
        for proposal in design_proposals:
            proposal_type = proposal.get('type', 'enhancement')
            content_type = f'design_{proposal_type}'
            
            score_result = self.assess_reasoning_quality(proposal, content_type)
            design_scores.append(score_result)
        
        # ç»Ÿè®¡åˆ†æ
        overall_qa_score = sum(s['overall_score'] for s in qa_scores) / len(qa_scores) if qa_scores else 0
        overall_design_score = sum(s['overall_score'] for s in design_scores) / len(design_scores) if design_scores else 0
        
        # è´¨é‡ç­‰çº§åˆ†å¸ƒ
        qa_quality_distribution = self._get_quality_distribution([s['overall_score'] for s in qa_scores])
        design_quality_distribution = self._get_quality_distribution([s['overall_score'] for s in design_scores])
        
        # ä¸è¾¾æ ‡å†…å®¹ç»Ÿè®¡
        failing_qa_count = sum(1 for s in qa_scores if not s['passes_threshold'])
        failing_design_count = sum(1 for s in design_scores if not s['passes_threshold'])
        
        return {
            'overall_summary': {
                'total_qa_pairs': len(qa_pairs),
                'total_design_proposals': len(design_proposals),
                'overall_qa_score': round(overall_qa_score, 3),
                'overall_design_score': round(overall_design_score, 3),
                'combined_score': round((overall_qa_score + overall_design_score) / 2, 3)
            },
            'qa_analysis': {
                'average_score': round(overall_qa_score, 3),
                'quality_distribution': qa_quality_distribution,
                'failing_count': failing_qa_count,
                'failing_rate': round(failing_qa_count / len(qa_scores), 3) if qa_scores else 0,
                'type_breakdown': {k: round(sum(v) / len(v), 3) for k, v in qa_type_scores.items()}
            },
            'design_analysis': {
                'average_score': round(overall_design_score, 3),
                'quality_distribution': design_quality_distribution,
                'failing_count': failing_design_count,
                'failing_rate': round(failing_design_count / len(design_scores), 3) if design_scores else 0
            },
            'detailed_scores': {
                'qa_detailed': qa_scores,
                'design_detailed': design_scores
            },
            'recommendations': self._generate_recommendations(
                overall_qa_score, overall_design_score, 
                failing_qa_count, failing_design_count
            )
        }
    
    def _get_quality_distribution(self, scores: List[float]) -> Dict[str, int]:
        """è·å–è´¨é‡ç­‰çº§åˆ†å¸ƒ"""
        if not scores:
            return {'excellent': 0, 'good': 0, 'fair': 0, 'poor': 0}
        
        distribution = {'excellent': 0, 'good': 0, 'fair': 0, 'poor': 0}
        
        for score in scores:
            if score >= 0.9:
                distribution['excellent'] += 1
            elif score >= 0.8:
                distribution['good'] += 1
            elif score >= 0.7:
                distribution['fair'] += 1
            else:
                distribution['poor'] += 1
        
        return distribution
    
    def _generate_recommendations(self, qa_score: float, design_score: float, 
                                failing_qa: int, failing_design: int) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if qa_score < 0.7:
            recommendations.append("QAæ¨ç†è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–promptä¸­çš„æ¨ç†æ¡†æ¶æŒ‡å¯¼")
        
        if design_score < 0.7:
            recommendations.append("è®¾è®¡æ–¹æ¡ˆæ¨ç†è´¨é‡åä½ï¼Œå»ºè®®å¢å¼ºæ¶æ„åˆ†æçš„æ·±åº¦è¦æ±‚")
        
        if failing_qa > 0:
            recommendations.append(f"æœ‰{failing_qa}ä¸ªQAå¯¹çš„æ¨ç†è´¨é‡ä¸è¾¾æ ‡ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")
        
        if failing_design > 0:
            recommendations.append(f"æœ‰{failing_design}ä¸ªè®¾è®¡æ–¹æ¡ˆçš„æ¨ç†è´¨é‡ä¸è¾¾æ ‡ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")
        
        if qa_score >= 0.8 and design_score >= 0.8:
            recommendations.append("æ¨ç†è´¨é‡æ•´ä½“è‰¯å¥½ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥æå‡å¤æ‚åº¦å’Œæ·±åº¦")
        
        return recommendations
    
    def save_quality_report(self, report: Dict[str, Any], output_path: str):
        """ä¿å­˜è´¨é‡æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š æ¨ç†è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")