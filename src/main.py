"""
æ™ºèƒ½è®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿ - ä¸»ç¨‹åº
åŸºäºClaude AIä¸ºä»£ç ä»“åº“ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®
"""
import os
import json
import math
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

from code_analyzer import CodeAnalyzer
from qa_generator import QAGenerator
from design_generator import DesignGenerator
from reasoning_quality_assessor import ReasoningQualityAssessor


class TrainingDataGenerator:
    """æ™ºèƒ½è®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, repo_path: str, output_dir: str, claude_api_key: str):
        self.repo_path = Path(repo_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        print("åˆå§‹åŒ–Claude AIå¢å¼ºçš„è®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿ...")
        self.analyzer = CodeAnalyzer(str(self.repo_path))
        self.qa_generator = QAGenerator(claude_api_key)
        self.design_generator = DesignGenerator(claude_api_key)
        self.quality_assessor = ReasoningQualityAssessor()
        
        self.analysis_result = None
        
    def run_full_pipeline(self, num_qa_pairs: int = 50, num_design_proposals: int = 10,
                         custom_requirements: Optional[List[str]] = None) -> Dict[str, str]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ç”Ÿæˆæµæ°´çº¿"""
        print(" å¯åŠ¨æ™ºèƒ½è®­ç»ƒæ•°æ®ç”Ÿæˆæµæ°´çº¿...")
        print(f" åˆ†æä»“åº“: {self.repo_path}")
        print(f" ç›®æ ‡: {num_qa_pairs} ä¸ªé—®ç­”å¯¹, {num_design_proposals} ä¸ªè®¾è®¡æ–¹æ¡ˆ")
        
        # Step 1: ä»£ç ä»“åˆ†æ
        print("\n Step 1: åˆ†æä»£ç ä»“åº“...")
        self.analysis_result = self._analyze_repository()
        
        # Step 2: ç”Ÿæˆé—®ç­”å¯¹
        print(f"\nâ“ Step 2: ç”Ÿæˆ {num_qa_pairs} ä¸ªé—®ç­”å¯¹...")
        qa_output_path = self._generate_qa_pairs(num_qa_pairs)
        
        # Step 3: ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ
        print(f"\n Step 3: ç”Ÿæˆ {num_design_proposals} ä¸ªè®¾è®¡æ–¹æ¡ˆ...")
        design_output_path = self._generate_design_proposals(num_design_proposals, custom_requirements)
        
        # Step 4: ç”Ÿæˆè®­ç»ƒæ•°æ®é›†
        print("\n Step 4: åˆ›å»ºæ ‡å‡†è®­ç»ƒæ•°æ®é›†...")
        dataset_path = self._create_training_dataset(qa_output_path, design_output_path)
        
        # Step 5: æ¨ç†è´¨é‡è¯„ä¼°
        print("\nğŸ” Step 5: è¯„ä¼°æ¨ç†è´¨é‡...")
        quality_report_path = self._assess_reasoning_quality(qa_output_path, design_output_path)
        
        # Step 6: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("\nStep 6: ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        report_path = self._generate_comprehensive_report()
        
        print("\nè®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ!")
        
        return {
            'analysis_report': str(self.output_dir / 'analysis_report.json'),
            'qa_pairs': qa_output_path,
            'design_proposals': design_output_path,
            'training_dataset': dataset_path,
            'quality_report': quality_report_path,
            'comprehensive_report': report_path
        }
    
    def _analyze_repository(self) -> Dict[str, Any]:
        """åˆ†æä»£ç ä»“"""
        analysis_result = self.analyzer.analyze_repository()
        
        # ä¿å­˜åˆ†æç»“æœ
        output_path = self.output_dir / 'analysis_report.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False)
            
        print(f"    å·²åˆ†æ {analysis_result['repo_structure']['total_files']} ä¸ªæ–‡ä»¶")
        print(f"    ç›®å½•æ·±åº¦: {analysis_result['repo_structure']['depth']}")
        detected_patterns = [k for k, v in analysis_result['architecture_patterns'].items() if v]
        if detected_patterns:
            print(f"   ğŸ” æ£€æµ‹åˆ°æ¶æ„æ¨¡å¼: {', '.join(detected_patterns)}")
        
        return analysis_result
    
    def _generate_qa_pairs(self, num_pairs: int) -> str:
        """ç”Ÿæˆé—®ç­”å¯¹"""
        qa_pairs = self.qa_generator.generate_qa_pairs(self.analysis_result, num_pairs)
        
        # ä¿å­˜é—®ç­”å¯¹
        output_path = str(self.output_dir / 'qa_pairs.json')
        self.qa_generator.save_qa_pairs(qa_pairs, output_path)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self._generate_qa_statistics(qa_pairs)
        print(f"    ç”Ÿæˆç»Ÿè®¡: {stats}")
        
        return output_path
    
    def _generate_design_proposals(self, num_proposals: int, 
                                 custom_requirements: Optional[List[str]] = None) -> str:
        """ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆ"""
        # ä½¿ç”¨è‡ªå®šä¹‰éœ€æ±‚æˆ–é»˜è®¤éœ€æ±‚
        requirements = custom_requirements or [
            'ç”¨æˆ·èº«ä»½éªŒè¯å’Œæˆæƒ',
            'å®æ—¶æ•°æ®å¤„ç†',
            'APIå®‰å…¨é˜²æŠ¤å’Œé™æµ',
            'ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–',
            'é”™è¯¯å¤„ç†å’Œæ—¥å¿—ç³»ç»Ÿ',
            'æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–',
            'å¾®æœåŠ¡æ¶æ„è¿ç§»',
            'å‰ç«¯ç»„ä»¶åº“å»ºè®¾',
            'è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶',
            'ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ'
        ]
        
        proposals = self.design_generator.generate_design_proposals(
            self.analysis_result, requirements, num_proposals
        )
        
        # ä¿å­˜è®¾è®¡æ–¹æ¡ˆ
        output_path = str(self.output_dir / 'design_proposals.json')
        self.design_generator.save_design_proposals(proposals, output_path)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self._generate_design_statistics(proposals)
        print(f"    ç”Ÿæˆç»Ÿè®¡: {stats}")
        
        return output_path
    
    def _assess_reasoning_quality(self, qa_path: str, design_path: str) -> str:
        """è¯„ä¼°æ¨ç†è´¨é‡"""
        # åŠ è½½æ•°æ®
        with open(qa_path, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
            
        with open(design_path, 'r', encoding='utf-8') as f:
            design_proposals = json.load(f)
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = self.quality_assessor.generate_quality_report(qa_pairs, design_proposals)
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        output_path = str(self.output_dir / 'reasoning_quality_report.json')
        self.quality_assessor.save_quality_report(quality_report, output_path)
        
        # æ‰“å°è´¨é‡æ‘˜è¦
        overall_summary = quality_report['overall_summary']
        print(f"    QAæ¨ç†è´¨é‡: {overall_summary['overall_qa_score']:.3f}")
        print(f"    è®¾è®¡æ¨ç†è´¨é‡: {overall_summary['overall_design_score']:.3f}")
        print(f"    ç»¼åˆæ¨ç†è´¨é‡: {overall_summary['combined_score']:.3f}")
        
        # æ˜¾ç¤ºè´¨é‡å»ºè®®
        recommendations = quality_report.get('recommendations', [])
        if recommendations:
            print("    è´¨é‡æ”¹è¿›å»ºè®®:")
            for rec in recommendations:
                print(f"      - {rec}")
        
        return output_path
    
    def _create_training_dataset(self, qa_path: str, design_path: str) -> str:
        """åˆ›å»ºæ ‡å‡†æ ¼å¼çš„è®­ç»ƒæ•°æ®é›†"""
        # åŠ è½½é—®ç­”å¯¹å’Œè®¾è®¡æ–¹æ¡ˆ
        with open(qa_path, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
            
        with open(design_path, 'r', encoding='utf-8') as f:
            design_proposals = json.load(f)
        
        # è½¬æ¢ä¸ºæ ‡å‡†è®­ç»ƒæ ¼å¼
        training_data = []
        
        # å¤„ç†é—®ç­”å¯¹
        for qa in qa_pairs:
            training_item = {
                'input': qa['question'],
                'output': qa['answer'],
                'context': qa['code_context'],
                'reasoning': qa['reasoning_trace'],
                'metadata': qa['metadata'],
                'type': 'qa_pair'
            }
            training_data.append(training_item)
        
        # å¤„ç†è®¾è®¡æ–¹æ¡ˆ
        for proposal in design_proposals:
            training_item = {
                'input': f"è¯·ä¸ºä»¥ä¸‹éœ€æ±‚è®¾è®¡è§£å†³æ–¹æ¡ˆ: {proposal['title']}",
                'output': proposal['description'],
                'context': proposal.get('design_approach', ''),
                'reasoning': proposal['reasoning_trace'],
                'metadata': proposal['metadata'],
                'type': 'design_proposal'
            }
            training_data.append(training_item)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®é›†
        output_path = str(self.output_dir / 'training_dataset.jsonl')
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in training_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"   ğŸ“¦ åˆ›å»ºäº†åŒ…å« {len(training_data)} æ¡è®°å½•çš„è®­ç»ƒæ•°æ®é›†")
        return output_path
    
    def _generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        report = {
            'project_overview': {
                'repository_path': str(self.repo_path),
                'analysis_timestamp': datetime.now().isoformat(),
                'total_files': self.analysis_result['repo_structure']['total_files'],
                'file_types': self.analysis_result['repo_structure']['file_types'],
                'architecture_patterns': self.analysis_result['architecture_patterns'],
                'technologies': self._detect_technologies()
            },
            'data_generation_summary': {
                'qa_pairs_generated': len(self._load_qa_pairs()),
                'design_proposals_generated': len(self._load_design_proposals()),
                'total_training_items': len(self._load_qa_pairs()) + len(self._load_design_proposals())
            },
            'quality_metrics': self._calculate_quality_metrics(),
            'recommendations': self._generate_recommendations(),
            'next_steps': [
                "ä½¿ç”¨ç”Ÿæˆçš„training_dataset.jsonlè®­ç»ƒæ‚¨çš„æ¨¡å‹",
                "æ ¹æ®é¡¹ç›®ç‰¹ç‚¹è°ƒæ•´é—®ç­”å¯¹æ•°é‡å’Œè®¾è®¡æ–¹æ¡ˆæ•°é‡",
                "è€ƒè™‘æ·»åŠ æ›´å¤šè‡ªå®šä¹‰éœ€æ±‚ä»¥è·å¾—æ›´é’ˆå¯¹æ€§çš„è®¾è®¡æ–¹æ¡ˆ",
                "å®šæœŸæ›´æ–°è®­ç»ƒæ•°æ®ä»¥åæ˜ ä»£ç åº“çš„å˜åŒ–"
            ]
        }
        
        output_path = str(self.output_dir / 'comprehensive_report.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        return output_path
    
    def _generate_qa_statistics(self, qa_pairs: List[Dict[str, Any]]) -> Dict[str, int]:
        """ç”Ÿæˆé—®ç­”å¯¹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for qa in qa_pairs:
            qtype = qa['metadata'].get('question_type', 'unknown')
            stats[qtype] = stats.get(qtype, 0) + 1
        return stats
    
    def _generate_design_statistics(self, proposals: List[Dict[str, Any]]) -> Dict[str, int]:
        """ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for proposal in proposals:
            ptype = proposal['metadata'].get('proposal_type', 'unknown')
            stats[ptype] = stats.get(ptype, 0) + 1
        return stats
    
    def _detect_technologies(self) -> List[str]:
        """æ£€æµ‹æŠ€æœ¯æ ˆ"""
        technologies = []
        file_types = self.analysis_result['repo_structure']['file_types']
        
        if '.py' in file_types:
            technologies.append('Python')
        if '.js' in file_types or '.ts' in file_types:
            technologies.append('JavaScript/TypeScript')
        if '.java' in file_types:
            technologies.append('Java')
        if '.go' in file_types:
            technologies.append('Go')
        if '.rs' in file_types:
            technologies.append('Rust')
        
        return technologies
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        doc_analysis = self.analysis_result['documentation_analysis']
        if not doc_analysis.get('has_readme'):
            recommendations.append("å»ºè®®æ·»åŠ README.mdæ–‡æ¡£")
        if not doc_analysis.get('has_license'):
            recommendations.append("å»ºè®®æ·»åŠ LICENSEæ–‡ä»¶")
        
        patterns = self.analysis_result['architecture_patterns']
        if not any(patterns.values()):
            recommendations.append("è€ƒè™‘é‡‡ç”¨æ›´æ¸…æ™°çš„æ¶æ„æ¨¡å¼")
        
        return recommendations
    
    def _calculate_quality_metrics(self) -> Dict[str, float]:
        """è®¡ç®—å®é™…çš„è´¨é‡æŒ‡æ ‡"""
        qa_pairs = self._load_qa_pairs()
        design_proposals = self._load_design_proposals()
        
        # 1. æ•°æ®å¤šæ ·æ€§å¾—åˆ†
        diversity_score = self._calculate_diversity_score(qa_pairs)
        
        # 2. ä»£ç è¦†ç›–ç‡å¾—åˆ†  
        coverage_score = self._calculate_code_coverage_score(qa_pairs)
        
        # 3. æ¨ç†è´¨é‡å¾—åˆ† (ä½¿ç”¨æ–°çš„è´¨é‡è¯„ä¼°å™¨)
        reasoning_score = self._calculate_enhanced_reasoning_quality_score(qa_pairs, design_proposals)
        
        # 4. å…ƒæ•°æ®å®Œæ•´æ€§å¾—åˆ†
        metadata_score = self._calculate_metadata_completeness(qa_pairs, design_proposals)
        
        # 5. æ•°æ®ä»£è¡¨æ€§å¾—åˆ†
        representativeness_score = self._calculate_representativeness_score(qa_pairs, design_proposals)
        
        return {
            'data_diversity_score': diversity_score,
            'code_coverage_score': coverage_score,
            'reasoning_quality_score': reasoning_score,
            'metadata_completeness': metadata_score,
            'data_representativeness_score': representativeness_score,
            'metric_definitions': {
                'data_diversity_score': 'åŸºäºé¦™å†œç†µçš„ä¿¡æ¯è®ºæŒ‡æ ‡ï¼Œè¯„ä¼°é—®é¢˜ç±»å‹ã€å¤æ‚åº¦ã€è§†è§’åˆ†å¸ƒå‡åŒ€æ€§(0-1)',
                'code_coverage_score': 'ç±»ä¼¼æµ‹è¯•è¦†ç›–ç‡æ¦‚å¿µï¼ŒåŸºäºä»£ç æ–‡ä»¶ã€å‡½æ•°ã€ç±»çš„è¦†ç›–ç¨‹åº¦(0-1)',
                'reasoning_quality_score': 'å‚è€ƒChain-of-Thoughtè¯„ä¼°æ¡†æ¶ï¼Œç»¼åˆé€»è¾‘ç»“æ„ã€å†…å®¹ç›¸å…³æ€§ã€æ·±åº¦åˆ†æ(0-1)',
                'metadata_completeness': 'åŸºäºæ•°æ®è´¨é‡è¯„ä¼°æ ‡å‡†ï¼Œè®¡ç®—å¿…è¦å…ƒæ•°æ®å­—æ®µå®Œæ•´ç¨‹åº¦(0-1)',
                'data_representativeness_score': 'åŸºäºç»Ÿè®¡å­¦ä»£è¡¨æ€§æŠ½æ ·åŸç†ï¼Œè¯„ä¼°ç”Ÿæˆæ•°æ®ä¸ç›®æ ‡ä»£ç åº“å®é™…ç‰¹å¾çš„ä¸€è‡´æ€§(0-1)'
            },
            'evaluation_methodology': {
                'diversity_method': 'Shannon Entropy (Information Theory)',
                'coverage_method': 'Code Coverage Analysis (Software Testing)',
                'reasoning_method': 'Custom Framework inspired by Chain-of-Thought Evaluation',
                'metadata_method': 'Data Quality Assessment Standards',
                'representativeness_method': 'Statistical Representativeness Analysis',
                'limitations': [
                    'æ¨ç†è´¨é‡è¯„ä¼°ä¸ºè‡ªå®šä¹‰æ–¹æ³•ï¼Œæœªç»å¤§è§„æ¨¡éªŒè¯',
                    'ç¼ºä¹ä¸æ ‡å‡†åŸºå‡†æ•°æ®é›†çš„å¯¹æ¯”',
                    'å…³é”®è¯åŒ¹é…æ–¹æ³•ç›¸å¯¹ç®€å•ï¼Œä¸å¦‚è¯­ä¹‰ç†è§£å‡†ç¡®',
                    'æƒé‡è®¾ç½®åŸºäºç»éªŒï¼Œç¼ºä¹ç†è®ºä¾æ®'
                ],
                'future_improvements': [
                    'å¼•å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—',
                    'ä¸GSM8Kã€StrategyQAç­‰æ ‡å‡†æ•°æ®é›†å¯¹æ¯”',
                    'ä½¿ç”¨äººå·¥è¯„ä¼°éªŒè¯è‡ªåŠ¨åŒ–è¯„ä¼°çš„å‡†ç¡®æ€§',
                    'åŸºäºå¤§è§„æ¨¡å®éªŒæ•°æ®ä¼˜åŒ–æƒé‡è®¾ç½®'
                ]
            }
        }
    
    def _calculate_diversity_score(self, qa_pairs: List[Dict]) -> float:
        """è®¡ç®—æ•°æ®å¤šæ ·æ€§å¾—åˆ†"""
        if not qa_pairs:
            return 0.0
            
        # ç»Ÿè®¡ä¸åŒç»´åº¦çš„åˆ†å¸ƒ
        question_types = {}
        complexity_levels = {}
        perspectives = {}
        
        for qa in qa_pairs:
            metadata = qa.get('metadata', {})
            
            # é—®é¢˜ç±»å‹åˆ†å¸ƒ
            qtype = metadata.get('question_type', 'unknown')
            question_types[qtype] = question_types.get(qtype, 0) + 1
            
            # å¤æ‚åº¦åˆ†å¸ƒ
            complexity = metadata.get('complexity_level', 'unknown')
            complexity_levels[complexity] = complexity_levels.get(complexity, 0) + 1
            
            # è§†è§’åˆ†å¸ƒ
            perspective = metadata.get('perspective', 'unknown')
            perspectives[perspective] = perspectives.get(perspective, 0) + 1
        
        # è®¡ç®—é¦™å†œç†µä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
        def shannon_entropy(distribution):
            total = sum(distribution.values())
            if total == 0:
                return 0
            entropy = 0
            for count in distribution.values():
                if count > 0:
                    p = count / total
                    entropy -= p * math.log2(p)
            return entropy
        
        # ç†è®ºæœ€å¤§ç†µï¼ˆå‡è®¾4ç§ç±»å‹ã€3ç§å¤æ‚åº¦ã€4ç§è§†è§’ï¼‰
        max_entropy_types = math.log2(4)  # log2(4) â‰ˆ 2.0
        max_entropy_complexity = math.log2(3)  # log2(3) â‰ˆ 1.58
        max_entropy_perspective = math.log2(4)  # log2(4) â‰ˆ 2.0
        
        # å®é™…ç†µ
        actual_entropy_types = shannon_entropy(question_types)
        actual_entropy_complexity = shannon_entropy(complexity_levels)
        actual_entropy_perspective = shannon_entropy(perspectives)
        
        # å½’ä¸€åŒ–å¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        diversity_score = (
            (actual_entropy_types / max_entropy_types) * 0.4 +
            (actual_entropy_complexity / max_entropy_complexity) * 0.3 +
            (actual_entropy_perspective / max_entropy_perspective) * 0.3
        )
        
        return round(min(diversity_score, 1.0), 3)
    
    def _calculate_code_coverage_score(self, qa_pairs: List[Dict]) -> float:
        """è®¡ç®—ä»£ç è¦†ç›–ç‡å¾—åˆ†"""
        if not qa_pairs:
            return 0.0
            
        # ç»Ÿè®¡è¢«è¦†ç›–çš„ä»£ç å…ƒç´ 
        covered_files = set()
        covered_functions = set()
        covered_classes = set()
        
        for qa in qa_pairs:
            metadata = qa.get('metadata', {})
            source_file = metadata.get('source_file', '')
            function_name = metadata.get('function_name', '')
            class_name = metadata.get('class_name', '')
            
            if source_file:
                covered_files.add(source_file)
            if function_name:
                covered_functions.add(f"{source_file}::{function_name}")
            if class_name:
                covered_classes.add(f"{source_file}::{class_name}")
        
        # è®¡ç®—æ€»çš„ä»£ç å…ƒç´ æ•°é‡
        total_files = self.analysis_result['repo_structure']['total_files']
        total_functions = sum(len(analysis.get('functions', [])) 
                            for analysis in self.analysis_result['file_analysis'].values())
        total_classes = sum(len(analysis.get('classes', [])) 
                          for analysis in self.analysis_result['file_analysis'].values())
        
        # è®¡ç®—è¦†ç›–ç‡
        file_coverage = len(covered_files) / max(total_files, 1)
        function_coverage = len(covered_functions) / max(total_functions, 1)
        class_coverage = len(covered_classes) / max(total_classes, 1)
        
        # åŠ æƒå¹³å‡è¦†ç›–ç‡
        coverage_score = (file_coverage * 0.3 + function_coverage * 0.4 + class_coverage * 0.3)
        
        return round(min(coverage_score, 1.0), 3)
    
    def _calculate_enhanced_reasoning_quality_score(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """ä½¿ç”¨æ–°çš„è´¨é‡è¯„ä¼°å™¨è®¡ç®—æ¨ç†è´¨é‡å¾—åˆ†"""
        if not qa_pairs and not design_proposals:
            return 0.0
        
        # ä½¿ç”¨æ–°çš„è´¨é‡è¯„ä¼°å™¨
        quality_report = self.quality_assessor.generate_quality_report(qa_pairs, design_proposals)
        combined_score = quality_report['overall_summary']['combined_score']
        
        # è¿”å›0-1èŒƒå›´çš„åˆ†æ•°
        return min(max(combined_score, 0.0), 1.0)
    
    def _calculate_reasoning_quality_score(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """è®¡ç®—æ¨ç†è´¨é‡å¾—åˆ† - åŸºäºé€»è¾‘ç»“æ„å’Œå†…å®¹è´¨é‡çš„ç»¼åˆè¯„ä¼°"""
        total_items = len(qa_pairs) + len(design_proposals)
        if total_items == 0:
            return 0.0
            
        quality_scores = []
        
        # è¯„ä¼°QAçš„æ¨ç†è´¨é‡
        for qa in qa_pairs:
            reasoning = qa.get('reasoning_trace', '')
            answer = qa.get('answer', '')
            question = qa.get('question', '')
            
            score = self._evaluate_reasoning_quality(reasoning, answer, question, 'qa')
            quality_scores.append(score)
        
        # è¯„ä¼°è®¾è®¡æ–¹æ¡ˆçš„æ¨ç†è´¨é‡
        for proposal in design_proposals:
            reasoning = proposal.get('reasoning_trace', '')
            description = proposal.get('description', '')
            title = proposal.get('title', '')
            
            score = self._evaluate_reasoning_quality(reasoning, description, title, 'proposal')
            quality_scores.append(score)
        
        return round(sum(quality_scores) / len(quality_scores), 3)
    
    def _evaluate_reasoning_quality(self, reasoning: str, content: str, title: str, content_type: str) -> float:
        """è¯„ä¼°å•ä¸ªæ¨ç†å†…å®¹çš„è´¨é‡"""
        if not reasoning or not content:
            return 0.0
            
        # 1. ç»“æ„å®Œæ•´æ€§è¯„ä¼° (40%)
        structure_score = self._evaluate_logical_structure(reasoning)
        
        # 2. å†…å®¹ç›¸å…³æ€§è¯„ä¼° (30%)
        relevance_score = self._evaluate_content_relevance(reasoning, content, title)
        
        # 3. æ·±åº¦å’Œè¯¦ç»†ç¨‹åº¦è¯„ä¼° (20%)
        depth_score = self._evaluate_reasoning_depth(reasoning, content_type)
        
        # 4. è¯­è¨€è´¨é‡è¯„ä¼° (10%)
        language_score = self._evaluate_language_quality(reasoning)
        
        # ä¼˜åŒ–æƒé‡åˆ†é…ï¼Œæé«˜ç›¸å…³æ€§å’Œè¯­è¨€è´¨é‡æƒé‡
        total_score = (
            structure_score * 0.25 +   # é™ä½ç»“æ„è¦æ±‚æƒé‡
            relevance_score * 0.35 +   # æé«˜ç›¸å…³æ€§æƒé‡
            depth_score * 0.25 +       # é€‚åº¦é™ä½æ·±åº¦è¦æ±‚
            language_score * 0.15      # æé«˜è¯­è¨€è´¨é‡æƒé‡
        )
        
        return min(total_score, 1.0)
    
    def _evaluate_logical_structure(self, reasoning: str) -> float:
        """è¯„ä¼°æ¨ç†çš„é€»è¾‘ç»“æ„"""
        score = 0.0
        reasoning_lower = reasoning.lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„åˆ†ææ­¥éª¤
        analysis_patterns = [
            'åˆ†æ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æœ€å', 'ç»¼åˆ',
            '1)', '2)', '3)', 'æ­¥éª¤', 'è¿‡ç¨‹', 'é˜¶æ®µ'
        ]
        has_steps = sum(1 for pattern in analysis_patterns if pattern in reasoning_lower)
        step_score = min(has_steps / 2, 1.0)  # è‡³å°‘2ä¸ªæ­¥éª¤æ ‡è¯†å¾—æ»¡åˆ†
        
        # æ£€æŸ¥å› æœé€»è¾‘
        causal_patterns = ['å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'ç”±äº', 'å¯¼è‡´', 'ç»“æœ', 'åŸå› ']
        has_causality = sum(1 for pattern in causal_patterns if pattern in reasoning_lower)
        causal_score = min(has_causality / 1, 1.0)  # è‡³å°‘1ä¸ªå› æœå…³ç³»å¾—æ»¡åˆ†
        
        # æ£€æŸ¥æ¨ç†è¿æ¥è¯
        reasoning_connectors = ['è€ƒè™‘åˆ°', 'åŸºäº', 'æ ¹æ®', 'é‰´äº', 'ç»¼åˆè€ƒè™‘', 'æƒè¡¡']
        has_connectors = sum(1 for connector in reasoning_connectors if connector in reasoning_lower)
        connector_score = min(has_connectors / 1, 1.0)  # è‡³å°‘1ä¸ªè¿æ¥è¯å¾—æ»¡åˆ†
        
        # æ£€æŸ¥ç»“è®ºæ€§è¯­å¥
        conclusion_patterns = ['æ€»ç»“', 'ç»“è®º', 'ç»¼ä¸Š', 'å› æ­¤å¯ä»¥', 'æœ€ç»ˆ', 'å»ºè®®']
        has_conclusion = any(pattern in reasoning_lower for pattern in conclusion_patterns)
        conclusion_score = 1.0 if has_conclusion else 0.5
        
        # åŠ æƒè®¡ç®—ç»“æ„å¾—åˆ† - ä¼˜åŒ–åçš„æƒé‡åˆ†é…
        score = (step_score * 0.4 + causal_score * 0.2 + connector_score * 0.2 + conclusion_score * 0.2)
        
        return score
    
    def _evaluate_content_relevance(self, reasoning: str, content: str, title: str) -> float:
        """è¯„ä¼°æ¨ç†ä¸å†…å®¹çš„ç›¸å…³æ€§"""
        # æå–å…³é”®è¯è¿›è¡Œç›¸å…³æ€§åˆ†æ
        def extract_keywords(text):
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
            import re
            words = re.findall(r'\b[\u4e00-\u9fa5a-zA-Z]{2,}\b', text.lower())
            return set(words)
        
        reasoning_keywords = extract_keywords(reasoning)
        content_keywords = extract_keywords(content)
        title_keywords = extract_keywords(title)
        
        # è®¡ç®—å…³é”®è¯é‡å åº¦
        if not reasoning_keywords:
            return 0.0
            
        content_overlap = len(reasoning_keywords & content_keywords) / len(reasoning_keywords)
        title_overlap = len(reasoning_keywords & title_keywords) / len(reasoning_keywords) if title_keywords else 0
        
        # ç»¼åˆç›¸å…³æ€§å¾—åˆ†
        relevance_score = content_overlap * 0.7 + title_overlap * 0.3
        
        return min(relevance_score, 1.0)
    
    def _evaluate_reasoning_depth(self, reasoning: str, content_type: str) -> float:
        """è¯„ä¼°æ¨ç†çš„æ·±åº¦å’Œè¯¦ç»†ç¨‹åº¦"""
        # é•¿åº¦è¯„ä¼°ï¼ˆåˆç†èŒƒå›´å†…ï¼‰
        min_length = 100 if content_type == 'qa' else 200  # é™ä½æœ€ä½é•¿åº¦è¦æ±‚
        optimal_length = 250 if content_type == 'qa' else 400  # é™ä½æœ€ä¼˜é•¿åº¦è¦æ±‚
        
        length = len(reasoning)
        if length < min_length:
            length_score = length / min_length
        elif length <= optimal_length:
            length_score = 1.0
        else:
            # è¶…è¿‡æœ€ä¼˜é•¿åº¦åå¾—åˆ†ç•¥å¾®ä¸‹é™ï¼Œé¿å…å†—é•¿
            length_score = max(0.8, optimal_length / length)
        
        # ç»†èŠ‚ä¸°å¯Œåº¦è¯„ä¼°
        detail_indicators = [
            'å…·ä½“', 'è¯¦ç»†', 'ä¾‹å¦‚', 'æ¯”å¦‚', 'åŒ…æ‹¬', 'æ¶‰åŠ', 'æ–¹é¢', 'å±‚é¢',
            'æ–¹æ³•', 'æ­¥éª¤', 'æµç¨‹', 'æœºåˆ¶', 'ç­–ç•¥', 'æ–¹æ¡ˆ', 'å®ç°', 'æŠ€æœ¯'
        ]
        detail_count = sum(1 for indicator in detail_indicators if indicator in reasoning)
        detail_score = min(detail_count / 3, 1.0)  # è‡³å°‘3ä¸ªç»†èŠ‚æŒ‡æ ‡å¾—æ»¡åˆ†
        
        # æ·±åº¦æ€è€ƒæŒ‡æ ‡
        depth_indicators = [
            'æƒè¡¡', 'å¯¹æ¯”', 'ä¼˜ç¼ºç‚¹', 'é£é™©', 'æŒ‘æˆ˜', 'é™åˆ¶', 'å½±å“', 'åæœ',
            'æ›¿ä»£æ–¹æ¡ˆ', 'æœ€ä½³å®è·µ', 'ç»éªŒ', 'æ•™è®­', 'åŸåˆ™', 'æ ‡å‡†'
        ]
        depth_count = sum(1 for indicator in depth_indicators if indicator in reasoning)
        depth_thinking_score = min(depth_count / 2, 1.0)  # è‡³å°‘2ä¸ªæ·±åº¦æ€è€ƒæŒ‡æ ‡å¾—æ»¡åˆ†
        
        # ç»¼åˆæ·±åº¦å¾—åˆ†
        depth_score = length_score * 0.4 + detail_score * 0.3 + depth_thinking_score * 0.3
        
        return depth_score
    
    def _evaluate_language_quality(self, reasoning: str) -> float:
        """è¯„ä¼°è¯­è¨€è´¨é‡å’Œè¡¨è¾¾æ¸…æ™°åº¦"""
        if not reasoning:
            return 0.0
            
        # åŸºæœ¬è¯­è¨€è´¨é‡æ£€æŸ¥
        sentences = reasoning.split('ã€‚')
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 5]
        
        if not valid_sentences:
            return 0.0
        
        # å¥å­é•¿åº¦åˆ†å¸ƒï¼ˆé¿å…è¿‡é•¿æˆ–è¿‡çŸ­çš„å¥å­ï¼‰
        sentence_lengths = [len(s) for s in valid_sentences]
        avg_length = sum(sentence_lengths) / len(sentence_lengths)
        
        # ç†æƒ³å¥å­é•¿åº¦åœ¨20-60å­—ä¹‹é—´
        if 20 <= avg_length <= 60:
            length_quality = 1.0
        elif avg_length < 20:
            length_quality = avg_length / 20
        else:
            length_quality = max(0.5, 60 / avg_length)
        
        # è¡¨è¾¾æ¸…æ™°åº¦ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰æ¸…æ™°çš„è¡¨è¾¾ï¼‰
        clarity_indicators = ['æ˜ç¡®', 'æ¸…æ¥š', 'æ˜¾ç„¶', 'å¯ä»¥çœ‹å‡º', 'è¡¨æ˜', 'è¯´æ˜', 'è¯æ˜']
        clarity_count = sum(1 for indicator in clarity_indicators if indicator in reasoning)
        clarity_score = min(clarity_count / 2, 1.0)
        
        # ä¸“ä¸šæ€§è¯„ä¼°
        professional_terms = [
            'ç³»ç»Ÿ', 'æ¶æ„', 'è®¾è®¡', 'å®ç°', 'ä¼˜åŒ–', 'æ€§èƒ½', 'å®‰å…¨', 'å¯ç»´æŠ¤',
            'æ‰©å±•', 'é›†æˆ', 'æ¥å£', 'æ¨¡å—', 'ç»„ä»¶', 'æœåŠ¡', 'æ¡†æ¶', 'æ¨¡å¼'
        ]
        professional_count = sum(1 for term in professional_terms if term in reasoning)
        professional_score = min(professional_count / 3, 1.0)
        
        # ç»¼åˆè¯­è¨€è´¨é‡å¾—åˆ†
        language_score = length_quality * 0.4 + clarity_score * 0.3 + professional_score * 0.3
        
        return language_score
    
    def _calculate_metadata_completeness(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """è®¡ç®—å…ƒæ•°æ®å®Œæ•´æ€§å¾—åˆ†"""
        total_items = len(qa_pairs) + len(design_proposals)
        if total_items == 0:
            return 0.0
            
        completeness_scores = []
        
        # QAå¿…è¦å­—æ®µ
        qa_required_fields = ['source_file', 'question_type', 'complexity_level', 'perspective', 'element_type']
        
        for qa in qa_pairs:
            metadata = qa.get('metadata', {})
            present_fields = sum(1 for field in qa_required_fields if metadata.get(field))
            completeness = present_fields / len(qa_required_fields)
            completeness_scores.append(completeness)
        
        # è®¾è®¡æ–¹æ¡ˆå¿…è¦å­—æ®µ  
        proposal_required_fields = ['proposal_type', 'complexity', 'generated_by']
        
        for proposal in design_proposals:
            metadata = proposal.get('metadata', {})
            present_fields = sum(1 for field in proposal_required_fields if metadata.get(field))
            completeness = present_fields / len(proposal_required_fields)
            completeness_scores.append(completeness)
        
        return round(sum(completeness_scores) / len(completeness_scores), 3)
    
    def _calculate_representativeness_score(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """è®¡ç®—æ•°æ®ä»£è¡¨æ€§å¾—åˆ†"""
        total_items = len(qa_pairs) + len(design_proposals)
        if total_items == 0:
            return 0.0
            
        scores = []
        
        # 1. æŠ€æœ¯æ ˆä¸€è‡´æ€§è¯„ä¼° (æƒé‡30%)
        tech_score = self._evaluate_tech_stack_consistency(qa_pairs, design_proposals)
        scores.append(tech_score * 0.3)
        
        # 2. ä¸šåŠ¡åœºæ™¯ç›¸å…³æ€§è¯„ä¼° (æƒé‡25%)  
        business_score = self._evaluate_business_relevance(qa_pairs, design_proposals)
        scores.append(business_score * 0.25)
        
        # 3. ä»£ç ä¸Šä¸‹æ–‡å‡†ç¡®æ€§è¯„ä¼° (æƒé‡25%)
        context_score = self._evaluate_code_context_accuracy(qa_pairs)
        scores.append(context_score * 0.25)
        
        # 4. æ¶æ„æ¨¡å¼åŒ¹é…åº¦è¯„ä¼° (æƒé‡20%)
        arch_score = self._evaluate_architecture_consistency(design_proposals)
        scores.append(arch_score * 0.2)
        
        return round(sum(scores), 3)
    
    def _evaluate_tech_stack_consistency(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """è¯„ä¼°æŠ€æœ¯æ ˆä¸€è‡´æ€§"""
        # è·å–å®é™…æŠ€æœ¯æ ˆ
        actual_tech_stack = set()
        for file_analysis in self.analysis_result['file_analysis'].values():
            imports = file_analysis.get('imports', [])
            for imp in imports:
                # æå–ä¸»è¦æŠ€æœ¯æ ˆ
                if 'flask' in imp.lower():
                    actual_tech_stack.add('Flask')
                elif 'jwt' in imp.lower():
                    actual_tech_stack.add('JWT')  
                elif 'redis' in imp.lower():
                    actual_tech_stack.add('Redis')
                elif 'hashlib' in imp.lower():
                    actual_tech_stack.add('bcrypt/hashlib')
        
        # è¯„ä¼°é—®ç­”å¯¹ä¸­æŠ€æœ¯æ ˆçš„åŒ¹é…åº¦
        matched_items = 0
        total_items = len(qa_pairs) + len(design_proposals)
        
        for qa in qa_pairs:
            answer = qa.get('answer', '').lower()
            context = qa.get('code_context', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…æŠ€æœ¯æ ˆç›¸å…³å†…å®¹
            if any(tech.lower() in answer or tech.lower() in context 
                   for tech in actual_tech_stack):
                matched_items += 1
        
        # è¯„ä¼°è®¾è®¡æ–¹æ¡ˆä¸­æŠ€æœ¯æ ˆçš„åŒ¹é…åº¦
        for proposal in design_proposals:
            description = proposal.get('description', '').lower()
            if any(tech.lower() in description for tech in actual_tech_stack):
                matched_items += 1
                
        return matched_items / max(total_items, 1)
    
    def _evaluate_business_relevance(self, qa_pairs: List[Dict], design_proposals: List[Dict]) -> float:
        """è¯„ä¼°ä¸šåŠ¡åœºæ™¯ç›¸å…³æ€§"""
        # è·å–å®é™…ä¸šåŠ¡å…³é”®è¯
        actual_keywords = set()
        for file_analysis in self.analysis_result['file_analysis'].values():
            keywords = file_analysis.get('business_keywords', [])
            actual_keywords.update(keywords)
        
        matched_items = 0
        total_items = len(qa_pairs) + len(design_proposals)
        
        # è¯„ä¼°é—®ç­”å¯¹ä¸šåŠ¡ç›¸å…³æ€§
        for qa in qa_pairs:
            context = qa.get('code_context', '').lower()
            reasoning = qa.get('reasoning_trace', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®é™…ä¸šåŠ¡å…³é”®è¯
            keywords_found = sum(1 for keyword in actual_keywords 
                               if keyword.lower() in context or keyword.lower() in reasoning)
            if keywords_found >= 2:  # è‡³å°‘åŒ…å«2ä¸ªä¸šåŠ¡å…³é”®è¯
                matched_items += 1
        
        # è¯„ä¼°è®¾è®¡æ–¹æ¡ˆä¸šåŠ¡ç›¸å…³æ€§  
        for proposal in design_proposals:
            description = proposal.get('description', '').lower()
            reasoning = proposal.get('reasoning_trace', '').lower()
            
            keywords_found = sum(1 for keyword in actual_keywords
                               if keyword.lower() in description or keyword.lower() in reasoning)
            if keywords_found >= 2:
                matched_items += 1
                
        return matched_items / max(total_items, 1)
    
    def _evaluate_code_context_accuracy(self, qa_pairs: List[Dict]) -> float:
        """è¯„ä¼°ä»£ç ä¸Šä¸‹æ–‡å‡†ç¡®æ€§"""
        accurate_items = 0
        
        for qa in qa_pairs:
            metadata = qa.get('metadata', {})
            source_file = metadata.get('source_file', '')
            function_name = metadata.get('function_name', '')
            
            # æ£€æŸ¥å¼•ç”¨çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºåˆ†æç»“æœä¸­
            if source_file and source_file in self.analysis_result['file_analysis']:
                file_analysis = self.analysis_result['file_analysis'][source_file]
                
                # æ£€æŸ¥å¼•ç”¨çš„å‡½æ•°æ˜¯å¦å­˜åœ¨
                if function_name:
                    functions = file_analysis.get('functions', [])
                    function_names = [f.get('name') for f in functions]
                    if function_name in function_names:
                        accurate_items += 1
                else:
                    # æ²¡æœ‰å…·ä½“å‡½æ•°åä½†æ–‡ä»¶å­˜åœ¨ï¼Œç®—éƒ¨åˆ†å‡†ç¡®
                    accurate_items += 0.5
                    
        return accurate_items / max(len(qa_pairs), 1)
    
    def _evaluate_architecture_consistency(self, design_proposals: List[Dict]) -> float:
        """è¯„ä¼°æ¶æ„æ¨¡å¼åŒ¹é…åº¦"""
        if not design_proposals:
            return 1.0
            
        # è·å–å®é™…æ¶æ„æ¨¡å¼
        actual_patterns = [k for k, v in self.analysis_result['architecture_patterns'].items() if v]
        
        consistent_proposals = 0
        for proposal in design_proposals:
            description = proposal.get('description', '').lower()
            reasoning = proposal.get('reasoning_trace', '').lower()
            
            # æ£€æŸ¥è®¾è®¡æ–¹æ¡ˆæ˜¯å¦ç¬¦åˆç°æœ‰æ¶æ„æ¨¡å¼
            if any(pattern.lower() in description or pattern.lower() in reasoning 
                   for pattern in actual_patterns):
                consistent_proposals += 1
            
        return consistent_proposals / len(design_proposals)
    
    def _load_qa_pairs(self) -> List[Dict[str, Any]]:
        """åŠ è½½é—®ç­”å¯¹"""
        try:
            with open(self.output_dir / 'qa_pairs.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    
    def _load_design_proposals(self) -> List[Dict[str, Any]]:
        """åŠ è½½è®¾è®¡æ–¹æ¡ˆ"""
        try:
            with open(self.output_dir / 'design_proposals.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Claude AIé©±åŠ¨çš„æ™ºèƒ½è®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿ')
    parser.add_argument('--repo-path', required=True, help='è¦åˆ†æçš„ä»£ç ä»“åº“è·¯å¾„')
    parser.add_argument('--output-dir', default='./output', help='è¾“å‡ºç›®å½• (é»˜è®¤: ./output)')
    parser.add_argument('--num-qa-pairs', type=int, default=50, help='ç”Ÿæˆé—®ç­”å¯¹æ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--num-design-proposals', type=int, default=10, help='ç”Ÿæˆè®¾è®¡æ–¹æ¡ˆæ•°é‡ (é»˜è®¤: 10)')
    parser.add_argument('--claude-api-key', help='Claude APIå¯†é’¥ (ä¹Ÿå¯ä½¿ç”¨ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY)')
    parser.add_argument('--requirements', nargs='+', help='è‡ªå®šä¹‰éœ€æ±‚åˆ—è¡¨')
    
    args = parser.parse_args()
    
    # è·å–Claude APIå¯†é’¥
    claude_api_key = claude_api_key
    if not claude_api_key:
        print(" é”™è¯¯: éœ€è¦æä¾›Claude APIå¯†é’¥")
        print(" æ–¹æ³•1: --claude-api-key 'your-api-key'")
        print(" æ–¹æ³•2: export ANTHROPIC_API_KEY='your-api-key'")
        return
    
    # éªŒè¯ä»“åº“è·¯å¾„
    if not Path(args.repo_path).exists():
        print(f" é”™è¯¯: ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {args.repo_path}")
        return
    
    try:
        # åˆ›å»ºç”Ÿæˆå™¨
        generator = TrainingDataGenerator(
            repo_path=args.repo_path,
            output_dir=args.output_dir,
            claude_api_key=claude_api_key
        )
        
        # è¿è¡Œç”Ÿæˆæµæ°´çº¿
        results = generator.run_full_pipeline(
            num_qa_pairs=args.num_qa_pairs,
            num_design_proposals=args.num_design_proposals,
            custom_requirements=args.requirements
        )
        
        print("\n ç”Ÿæˆå®Œæˆ!")
        print("\n ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_type, file_path in results.items():
            if Path(file_path).exists():
                file_size = Path(file_path).stat().st_size
                print(f"    {file_type}: {file_path} ({file_size:,} bytes)")
        
        print(f"\n ä¸»è¦æ–‡ä»¶: {results['training_dataset']}")
        print(" è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†ç”¨äºæ¨¡å‹è®­ç»ƒçš„æ ‡å‡†æ ¼å¼æ•°æ®")
        
    except Exception as e:
        print(f" ç³»ç»Ÿè¿è¡Œå‡ºé”™: {e}")
        print(" è¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®ï¼Œç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")


if __name__ == "__main__":
    main()
